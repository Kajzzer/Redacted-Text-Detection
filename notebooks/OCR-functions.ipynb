{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa22f47d",
   "metadata": {},
   "source": [
    "## This document contains the functions from the [Experiments notebook](https://github.com/irlabamsterdam/TPDLTextRedaction/blob/main/notebooks/Experiments.ipynb) of the [TPDLTextRedaction repo](https://github.com/irlabamsterdam/TPDLTextRedaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4410f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import cv2, os, json, time, pytesseract, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# make sure we use the correct path to tesseract when we use windows\n",
    "if platform.system() == 'Windows':\n",
    "    pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f411574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply load the PNG images\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function that loads an image from a path.\n",
    "    :param image_path: string specifying the path to the image\n",
    "    :return: Numpy array with the image in BGR format.\n",
    "    \"\"\"\n",
    "    # Checking if it is an image\n",
    "    if image_path.lower().endswith('.png'):\n",
    "        # Load the image in BGR format\n",
    "        image = cv2.imread(image_path)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa2769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(image: np.ndarray, text_pre_closing_kernel_size: tuple = (2, 2),\n",
    "                      text_pre_guassian_blur_size: tuple = (3, 3)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param image: Numpy array representing the input image in BGR format.\n",
    "    :return: Numpy array with a grayscale image after applied operations.\n",
    "    Method that applies image preprocessing for input to Tesseract, performs the following\n",
    "    operations:\n",
    "    1. Conversion of the iamge to grayscale\n",
    "    2. Closing of the image with a 2 by 2 kernel to remove noise.\n",
    "    3. Guassian blur with a 3 by 3 kernel.\n",
    "    \"\"\"\n",
    "    # First we convert the input image to grayscale\n",
    "    image_grayscale = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # We set up a kernel for the closing operation\n",
    "    kernel = np.ones(text_pre_closing_kernel_size, np.uint8)\n",
    "    \n",
    "    # we perform closing, i.e. dilation followed by erosion\n",
    "    closed_image = cv2.morphologyEx(image_grayscale, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Finally we use a Guassian blur over the image with a 3 by 3 kernel size\n",
    "    image_blurred = cv2.GaussianBlur(closed_image, ksize=text_pre_guassian_blur_size, sigmaX=0)\n",
    "\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabc34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redaction_box_preprocessing(image: np.ndarray, box_pre_horizontal_closing_size: tuple = (1, 3),\n",
    "                               box_pre_vertical_closing_size: tuple=(3, 1),\n",
    "                               box_pre_bilat_filter_size: int = 5,\n",
    "                               box_pre_filter_sigma_color: int = 75,\n",
    "                               box_pre_filter_sigma_space: int=75) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param image: Numpy array representing the input image in BGR format.\n",
    "    :return: Numpy array with a grayscale image after applied operations.\n",
    "    Method that applies image preprocessing for input the morphological operations, performs the following\n",
    "    operations:\n",
    "    1. Conversion of the iamge to grayscale\n",
    "    2. Horizontal opening with a 1 by 3 kernel\n",
    "    3. Vertical opening with a 3 by 1 kernel\n",
    "    3. Bilateral filter.\n",
    "    \"\"\"\n",
    "    # First we convert the input image to grayscale\n",
    "    image_grayscale = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # We perform two sets of opening operations, a horizontal one, followed by a vertical one.\n",
    "    horizontal_kernel = np.ones(box_pre_horizontal_closing_size, np.uint8)\n",
    "    horizontally_opened_image = cv2.morphologyEx(image_grayscale, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    \n",
    "    # Apply kernel vertically over the horizontally opened image\n",
    "    vertical_kernel = np.ones(box_pre_vertical_closing_size, np.uint8)\n",
    "    vertically_opened_image = cv2.morphologyEx(horizontally_opened_image, cv2.MORPH_OPEN, vertical_kernel)\n",
    "\n",
    "    # Perform a bilateral blur\n",
    "    bilateral_blurred_image = cv2.bilateralFilter(vertically_opened_image, box_pre_bilat_filter_size,\n",
    "                                                  box_pre_filter_sigma_color, box_pre_filter_sigma_space)\n",
    "\n",
    "    return bilateral_blurred_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c63555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(text_image: np.ndarray, redaction_box_image: np.ndarray,\n",
    "               tesseract_confidence: int = 65):\n",
    "    # Count the total number of pixes of the pages occupied by words\n",
    "    words_area = 0\n",
    "    # Make a copy of the image where we will apply our transformation to.\n",
    "    image_without_text = redaction_box_image.copy()\n",
    "    # Get the width and height of the images\n",
    "    image_height, image_width = redaction_box_image.shape[:2]\n",
    "\n",
    "    # Set up the code to detect the leftmost and rightmost pieces of a page.\n",
    "    left_boundary = [image_width]\n",
    "    right_boundary = [0]\n",
    "    top_boundary = [image_height]\n",
    "    bottom_boundary = [0]\n",
    "\n",
    "    # Specify the codes we want to detect\n",
    "    codes = ['5.1.1.', '5.1.2.']\n",
    "\n",
    "    # run tesseract on the image preprocessed for text\n",
    "    tesseract_output = pytesseract.image_to_data(text_image, lang='nld+eng', output_type=Output.DICT)\n",
    "    \n",
    "    # Get the height of the text\n",
    "    height = np.array(tesseract_output['height'])\n",
    "    # get the median height of the text, we will use this to calculate\n",
    "    # how much of the page is occupied by words\n",
    "    median_text_height = np.median(height[height < 0.3*image_height])\n",
    "\n",
    "    # Get the number of detected text pages\n",
    "    number_of_boxes = len(tesseract_output['level'])\n",
    "    for box in range(number_of_boxes):\n",
    "        # Get the coordinates of the text box if it actually contain any text\n",
    "        if (tesseract_output['text'][box].strip() != \"\") and (tesseract_output['conf'][box] != -1) and (tesseract_output['height'][box] < 0.3*image_height):\n",
    "            (x, y, w, h) = (tesseract_output['left'][box], tesseract_output['top'][box], tesseract_output['width'][box], tesseract_output['height'][box])\n",
    "\n",
    "            # If the text contains one of the codes we want to keep it and not remove it\n",
    "            # from the page\n",
    "            if any([code in tesseract_output['text'][box] for code in codes]):\n",
    "                # If the text is longer we want to adjust the width to include more specific subcodes\n",
    "                if len(tesseract_output['text'][box]) > 7:\n",
    "                    sub_index = tesseract_output['text'][box].find('5.1.')\n",
    "                    char_width = w / len(tesseract_output['text'][box])\n",
    "                    w = int(char_width * 7)\n",
    "                    x += int(char_width * sub_index)\n",
    "                # make the redaction boxes a white color\n",
    "                cv2.rectangle(image_without_text, (x, y), (x + w, y + h), (0, 0, 0), -1)\n",
    "            # If its not a redaction box and the confidence is high enough\n",
    "            # remove the text from hte page\n",
    "            elif tesseract_output['conf'][box] > tesseract_confidence:\n",
    "                words_area += (w*h)\n",
    "                cv2.rectangle(image_without_text, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "                if median_text_height*1.1 > tesseract_output['height'][box] > median_text_height*0.9:\n",
    "                    left_boundary.append(x)\n",
    "                    right_boundary.append(x+w)\n",
    "                    top_boundary.append(y)\n",
    "                    bottom_boundary.append(y+h)\n",
    "\n",
    "    image_without_text = cv2.threshold(image_without_text, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    return image_without_text, words_area, {'left_boundary': min(left_boundary),\n",
    "                                'right_boundary': max(right_boundary),\n",
    "                                'top_boundary': min(top_boundary),\n",
    "                               'bottom_boundary': max(bottom_boundary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a96077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_contours(image_without_text: np.ndarray, contour_opening_kernel_size: tuple = (5, 5)):\n",
    "\n",
    "    # Find the contours we have so far and fill them so we can perform more operations on them\n",
    "    contours = cv2.findContours(image_without_text, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "    for contour in contours:\n",
    "        # filll contours with white\n",
    "        cv2.drawContours(image_without_text, [contour], -1, (255,255,255), -1)\n",
    "\n",
    "    # Here we remove noise by using an opening operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, contour_opening_kernel_size)\n",
    "    opened_image = cv2.morphologyEx(image_without_text, cv2.MORPH_OPEN, kernel, iterations=4)\n",
    "\n",
    "    # Draw rectangles\n",
    "    new_contours = cv2.findContours(opened_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    new_contours = new_contours[0] if len(new_contours) == 2 else new_contours[1]\n",
    "    \n",
    "    return opened_image, new_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_contours(original_image: np.ndarray, contours: list, text_boundaries: dict):\n",
    "    \n",
    "    final_image = original_image.copy()\n",
    "    final_contour_image = np.zeros([original_image.shape[0], original_image.shape[1]], dtype=np.uint8)\n",
    "    \n",
    "    # set thresholds for the sizes of the bounding boxes that we are going to keep\n",
    "    area_treshold_min = 0.000125 * original_image.shape[0] * original_image.shape[1]\n",
    "    area_treshold_max = 0.4 * original_image.shape[0] * original_image.shape[1]\n",
    "\n",
    "    left_text_boundary = [text_boundaries['left_boundary']]\n",
    "    right_text_boundary = [text_boundaries['right_boundary']]\n",
    "    top_text_boundary = [text_boundaries['top_boundary']]\n",
    "    bottom_text_boundary = [text_boundaries['bottom_boundary']]\n",
    "    \n",
    "    # boolean indicating if there is any redacted text on the page\n",
    "    redacted_bool = False\n",
    "    number_of_redacted_regions = 0\n",
    "    total_contour_area = 0\n",
    "    # save the final contours\n",
    "    final_contours = []\n",
    "\n",
    "    for contour in contours:\n",
    "        # Find extreme points of contours\n",
    "        contour_left = tuple(contour[contour[:, :, 0].argmin()][0])\n",
    "        contour_right = tuple(contour[contour[:, :, 0].argmax()][0])\n",
    "        contour_top = tuple(contour[contour[:, :, 1].argmin()][0])\n",
    "        contour_bottom = tuple(contour[contour[:, :, 1].argmax()][0])\n",
    "\n",
    "        # Filter out rectangles that are too small, or where the height is bigger than the width\n",
    "        if area_treshold_max > cv2.contourArea(contour) > area_treshold_min and ((contour_bottom[1] - contour_top[1]) < (contour_right[0] - contour_left[0])):\n",
    "            \n",
    "            final_contours.append(contour)\n",
    "            # add the contours into the final image\n",
    "            cv2.drawContours(final_image, [contour], -1, (0,255, 0), thickness=5)\n",
    "            cv2.drawContours(final_contour_image, [contour], -1, (255, 255, 255), -1)\n",
    "\n",
    "            left_text_boundary.append(contour_left[0])\n",
    "            right_text_boundary.append(contour_right[0])\n",
    "            top_text_boundary.append(contour_top[1])\n",
    "            bottom_text_boundary.append(contour_bottom[1])\n",
    "            \n",
    "            total_contour_area += cv2.contourArea(contour)\n",
    "            number_of_redacted_regions += 1\n",
    "\n",
    "    text_area = ((max(right_text_boundary) - min(left_text_boundary)) * (max(bottom_text_boundary) - min(top_text_boundary)))\n",
    "\n",
    "    \n",
    "    return final_image, final_contour_image, final_contours, total_contour_area, text_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c73bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(input_image_path: str,\n",
    "                  text_pre_closing_kernel_size: tuple = (2, 2),\n",
    "                  text_pre_guassian_blur_size: tuple = (3, 3),\n",
    "                  box_pre_horizontal_closing_size: tuple = (1, 3),\n",
    "                  box_pre_vertical_closing_size: tuple = (3, 1),\n",
    "                  box_pre_bilat_filter_size: int = 5,\n",
    "                  box_pre_filter_sigma_color: int = 75,\n",
    "                  box_pre_filter_sigma_space: int = 75,\n",
    "                  tesseract_confidence: int = 65,\n",
    "                  contour_opening_kernel_size: tuple = (5, 5)):\n",
    "    \"\"\"\n",
    "    This functions implements the complete redaction detection algorithm and contains the options\n",
    "    to set the parameters used as to experiment with different settings.\n",
    "    :param input_image_path: string specifying the path to the input image\n",
    "    :param text_pre_closing_kernel_size: size of the closing kernel for the text preprocessing step\n",
    "    :param text_pre_guassian_blur_size: size of the kernel for the Gaussian blur for the text\n",
    "    preprocessing step\n",
    "    :param box_pre_horizontal_closing_size: size of the horizontal closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_vertical_closing_size:size of the vertical closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_bilat_filter_size: Size of the bilateral filter kernel for the redaction box\n",
    "    preprocssing step.\n",
    "    :param box_pre_filter_sigma_color: color sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing step\n",
    "    :param box_pre_filter_sigma_space: space sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing ste\n",
    "    :param tesseract_confidence: integer specifying the confidence level for Tesseract to \n",
    "    consider something to be text\n",
    "    :param contour_opening_kernel_size: kernel size of the opening operation in the contour detection step.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_image = load_image(input_image_path)\n",
    "    # Do the preprocessing\n",
    "    image_text_pre = text_preprocessing(input_image, text_pre_closing_kernel_size)\n",
    "    \n",
    "    image_box_pre = redaction_box_preprocessing(input_image, \n",
    "                                                box_pre_horizontal_closing_size,\n",
    "                                                box_pre_vertical_closing_size,\n",
    "                                                box_pre_bilat_filter_size,\n",
    "                                                box_pre_filter_sigma_color,\n",
    "                                                box_pre_filter_sigma_space)\n",
    "    # Remove the text\n",
    "    image_without_text, total_words_area, text_boundaries = remove_text(image_text_pre, image_box_pre,\n",
    "                                                                       tesseract_confidence)\n",
    "    # First contour detection step\n",
    "    image_with_contours, contours = determine_contours(image_without_text, contour_opening_kernel_size)\n",
    "    # final contouring filtering step\n",
    "    final_image_with_contours, final_contour_image, final_contours, total_contour_area, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    \n",
    "    # Automatically calculate some statistics on the number of redacted boxes, and the total percentage of \n",
    "    # the page that is redacted.\n",
    "    # Check how much of the text area is redacted (%)\n",
    "    percentage_redacted_textarea = ((total_contour_area / total_text_area) * 100) if total_contour_area and total_text_area else 0\n",
    "\n",
    "    # Check how much of character area is redacted (%)\n",
    "    total_area = total_contour_area + total_words_area\n",
    "    percentage_redacted_words = ((total_contour_area / total_area) * 100) if total_contour_area else 0\n",
    "    num_of_redacted_regions = len(final_contours)\n",
    "    \n",
    "    return final_contours, percentage_redacted_words, num_of_redacted_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1dccddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_algorithm_steps(input_image_path):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(15, 10))\n",
    "    \n",
    "    input_image = load_image(input_image_path)\n",
    "    # Do the preprocessing\n",
    "    image_text_pre, image_box_pre = text_preprocessing(input_image), redaction_box_preprocessing(input_image)\n",
    "    # Remove the text\n",
    "    image_without_text, total_words_area, text_boundaries = remove_text(image_text_pre, image_box_pre)\n",
    "    # First contour detection step\n",
    "    image_with_contours, contours = determine_contours(image_without_text)\n",
    "    # final contouring filtering step\n",
    "    final_image_with_contours, final_contour_image, final_contours, total_contour_area, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    \n",
    "    axes[0].imshow(input_image)\n",
    "    axes[1].imshow(image_text_pre, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[2].imshow(image_box_pre, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[3].imshow(image_without_text, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[4].imshow(image_with_contours, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[5].imshow(final_contour_image, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[6].imshow(final_image_with_contours)\n",
    "    \n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[1].set_title(\"Text Preprocessing\")\n",
    "    axes[2].set_title(\"Redaction Box\\n Preprocessing\")\n",
    "    axes[3].set_title(\"Text Removal\")\n",
    "    axes[4].set_title(\"Contour Detection\")\n",
    "    axes[5].set_title(\"Contour Filtering\")\n",
    "    axes[6].set_title(\"Final Image \")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be57bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4e0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the files containing all the gold standard regions \n",
    "gold_standard = read_json(os.path.join('..', 'datasets', 'gold_standard_complete.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96398591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite this to just work with only the output polygons\n",
    "def evaluate_detection(input_image_filename):\n",
    "    gold_standard_contours = gold_standard[os.path.split(input_image_filename)[-1]]\n",
    "    predicted_contours, _, _ = run_algorithm(input_image_filename)\n",
    "    \n",
    "    # Get the attributes of all the boxes in the gold standard annotation\n",
    "    polygons = [r['shape_attributes'] for r in gold_standard_contours['regions']]\n",
    "\n",
    "    # set the total sum of the IOU\n",
    "    sum_IoU = 0\n",
    "    TP = []\n",
    "    FN = []\n",
    "    FP = []\n",
    "\n",
    "    predicted_polygons = [Polygon(np.squeeze(contour)) for contour in predicted_contours]\n",
    "    ground_truth_polygons = []\n",
    "    \n",
    "    for polygon in polygons:\n",
    "        if polygon['name'] == 'rect':\n",
    "            bottom_left = [polygon['x'], polygon['y']]\n",
    "            bottom_right = [polygon['x']+polygon['width'], polygon['y']]\n",
    "            top_right = [polygon['x']+polygon['width'], polygon['y']+polygon['height']]\n",
    "            top_left = [polygon['x'], polygon['y']+polygon['height']]\n",
    "            \n",
    "            gold_standard_polygon_xy = [bottom_left, bottom_right, top_right, top_left]\n",
    "        else:\n",
    "            # If not a rectangle we have a more complex shape and we just add all points to it\n",
    "            gold_standard_polygon_xy  = [[polygon['all_points_x'][i], polygon['all_points_y'][i]] for i in range(0, len(polygon['all_points_x']))]\n",
    "\n",
    "        ground_truth_polygons.append(Polygon(gold_standard_polygon_xy))\n",
    "        \n",
    "    # Loop over all of the predicted and ground truth polygons and check if there\n",
    "    # is enough overlap to be a True Positive\n",
    "    for predicted_polygon in predicted_polygons:\n",
    "        for ground_truth_polygon in ground_truth_polygons:\n",
    "            \n",
    "            # make sure that the polygone is not self-intersecting\n",
    "            if not ground_truth_polygon.is_valid:\n",
    "                ground_truth_polygon = ground_truth_polygon.buffer(0)\n",
    "                \n",
    "            polygon_intersection = predicted_polygon.intersection(ground_truth_polygon).area\n",
    "            polygon_union = predicted_polygon.area + ground_truth_polygon.area - polygon_intersection\n",
    "        \n",
    "            if (polygon_intersection / polygon_union) >= 0.5:\n",
    "                sum_IoU += (polygon_intersection / polygon_union)\n",
    "                TP.append([ground_truth_polygon, predicted_polygon])\n",
    "\n",
    "    # Calculate false positives, false negatives and true positives and the PQ\n",
    "    # score.\n",
    "    FP = [polygon for polygon in predicted_polygons if polygon not in [item[1] for item in TP]]\n",
    "    FN = [polygon for polygon in ground_truth_polygons if polygon not in [item[0] for item in TP]]\n",
    "    \n",
    "    return {'TP': len(TP), 'FP': len(FP), 'FN': len(FN), 'IOU': sum_IoU}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataframe(images_dir):\n",
    "    all_scores = {}\n",
    "    for filename in tqdm(os.listdir(images_dir)):\n",
    "        path = os.path.join(images_dir, filename)\n",
    "        image_scores = evaluate_detection(path)\n",
    "        all_scores[filename] = image_scores\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9bc9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculation(dataframe):\n",
    "    # We calculate the metrics over the total of each class, basically viewing the \n",
    "    # complete class as one big image.\n",
    "    \n",
    "    SQ = dataframe['IOU'].sum() / dataframe['TP'].sum()\n",
    "    RQ = dataframe['TP'].sum() / (dataframe['TP'].sum() + 0.5*dataframe['FN'].sum() + 0.5*dataframe['FP'].sum())\n",
    "    PQ = SQ*RQ\n",
    "    P = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FP'].sum())\n",
    "    R = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FN'].sum())\n",
    "    \n",
    "    number_of_segments =  dataframe['TP'] + dataframe['FN']\n",
    "    dataframe['num_segments'] = number_of_segments\n",
    "    return round(SQ, 2), round(RQ, 2) , round(PQ, 2), round(P, 2), round(R, 2), dataframe\n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
