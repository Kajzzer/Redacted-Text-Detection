{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66057b6b",
   "metadata": {},
   "source": [
    "## This document contains the functions from the [Experiments notebook](https://github.com/irlabamsterdam/TPDLTextRedaction/blob/main/notebooks/Experiments.ipynb) of the [TPDLTextRedaction repo](https://github.com/irlabamsterdam/TPDLTextRedaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4410f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the functions of the OCR model\n",
    "%run OCR-functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27dad2",
   "metadata": {},
   "source": [
    "# Make sure that we have the train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0e3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the functions to create the train and test split\n",
    "%run Data-split-functions.ipynb\n",
    "\n",
    "# create the data, set force_new_split to false so that we can skip \n",
    "# this phase if it has been done before and \n",
    "# extended to false to skip the 'no_annotation' pages\n",
    "split_data(TRAIN_SPLIT, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc28d4",
   "metadata": {},
   "source": [
    "# An extra function to run the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8857d50",
   "metadata": {},
   "source": [
    "We also provide a small function to run the algorithm and plot all steps, together with a complete function to run the algorithm that also contains all the parameters used in the functions so that you can experiment with different things yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c73bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(input_image_path: str,\n",
    "                  text_pre_closing_kernel_size: tuple = (2, 2),\n",
    "                  text_pre_guassian_blur_size: tuple = (3, 3),\n",
    "                  box_pre_horizontal_closing_size: tuple = (1, 3),\n",
    "                  box_pre_vertical_closing_size: tuple = (3, 1),\n",
    "                  box_pre_bilat_filter_size: int = 5,\n",
    "                  box_pre_filter_sigma_color: int = 75,\n",
    "                  box_pre_filter_sigma_space: int = 75,\n",
    "                  tesseract_confidence: int = 65,\n",
    "                  contour_opening_kernel_size: tuple = (5, 5)):\n",
    "    \"\"\"\n",
    "    This functions implements the complete redaction detection algorithm and contains the options\n",
    "    to set the parameters used as to experiment with different settings.\n",
    "    :param input_image_path: string specifying the path to the input image\n",
    "    :param text_pre_closing_kernel_size: size of the closing kernel for the text preprocessing step\n",
    "    :param text_pre_guassian_blur_size: size of the kernel for the Gaussian blur for the text\n",
    "    preprocessing step\n",
    "    :param box_pre_horizontal_closing_size: size of the horizontal closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_vertical_closing_size:size of the vertical closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_bilat_filter_size: Size of the bilateral filter kernel for the redaction box\n",
    "    preprocssing step.\n",
    "    :param box_pre_filter_sigma_color: color sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing step\n",
    "    :param box_pre_filter_sigma_space: space sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing ste\n",
    "    :param tesseract_confidence: integer specifying the confidence level for Tesseract to \n",
    "    consider something to be text\n",
    "    :param contour_opening_kernel_size: kernel size of the opening operation in the contour detection step.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_image = load_image(input_image_path)\n",
    "    # Do the preprocessing\n",
    "    image_text_pre = text_preprocessing(input_image, text_pre_closing_kernel_size)\n",
    "    \n",
    "    image_box_pre = redaction_box_preprocessing(input_image, \n",
    "                                                box_pre_horizontal_closing_size,\n",
    "                                                box_pre_vertical_closing_size,\n",
    "                                                box_pre_bilat_filter_size,\n",
    "                                                box_pre_filter_sigma_color,\n",
    "                                                box_pre_filter_sigma_space)\n",
    "    # Remove the text\n",
    "    image_without_text, total_words_area, text_boundaries = remove_text(image_text_pre, image_box_pre,\n",
    "                                                                       tesseract_confidence)\n",
    "    # First contour detection step\n",
    "    image_with_contours, contours = determine_contours(image_without_text, contour_opening_kernel_size)\n",
    "    # final contouring filtering step\n",
    "    final_image_with_contours, final_contour_image, final_contours, total_contour_area, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    \n",
    "    # Automatically calculate some statistics on the number of redacted boxes, and the total percentage of \n",
    "    # the page that is redacted.\n",
    "    # Check how much of the text area is redacted (%)\n",
    "    percentage_redacted_textarea = ((total_contour_area / total_text_area) * 100) if total_contour_area and total_text_area else 0\n",
    "\n",
    "    # Check how much of character area is redacted (%)\n",
    "    total_area = total_contour_area + total_words_area\n",
    "    percentage_redacted_words = ((total_contour_area / total_area) * 100) if total_contour_area else 0\n",
    "    num_of_redacted_regions = len(final_contours)\n",
    "    \n",
    "    return final_contours, percentage_redacted_words, num_of_redacted_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e48d2c",
   "metadata": {},
   "source": [
    "## Measuring model performance\n",
    "\n",
    "In this part of the notebook we will run the scores on the testsets and evaluate using PQ. For this, we will be using the `annotations.json` file, which contains the annotations of the redacted text contours of the images. After this we will also time the different parts of the algorithm so that we get a clear understanding of what parts of the algorithms are most time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be57bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4e0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the files containing all the gold standard regions \n",
    "gold_standard = read_json(os.path.join('..', 'datasets', 'gold_standard_complete.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96398591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite this to just work with only the output polygons\n",
    "def evaluate_detection(input_image_filename):\n",
    "    gold_standard_contours = gold_standard[os.path.split(input_image_filename)[-1]]\n",
    "    predicted_contours, _, _ = run_algorithm(input_image_filename)\n",
    "    \n",
    "    # Get the attributes of all the boxes in the gold standard annotation\n",
    "    polygons = [r['shape_attributes'] for r in gold_standard_contours['regions']]\n",
    "\n",
    "    # set the total sum of the IOU\n",
    "    sum_IoU = 0\n",
    "    TP = []\n",
    "    FN = []\n",
    "    FP = []\n",
    "\n",
    "    predicted_polygons = [Polygon(np.squeeze(contour)) for contour in predicted_contours]\n",
    "    ground_truth_polygons = []\n",
    "    \n",
    "    for polygon in polygons:\n",
    "        if polygon['name'] == 'rect':\n",
    "            bottom_left = [polygon['x'], polygon['y']]\n",
    "            bottom_right = [polygon['x']+polygon['width'], polygon['y']]\n",
    "            top_right = [polygon['x']+polygon['width'], polygon['y']+polygon['height']]\n",
    "            top_left = [polygon['x'], polygon['y']+polygon['height']]\n",
    "            \n",
    "            gold_standard_polygon_xy = [bottom_left, bottom_right, top_right, top_left]\n",
    "        else:\n",
    "            # If not a rectangle we have a more complex shape and we just add all points to it\n",
    "            gold_standard_polygon_xy  = [[polygon['all_points_x'][i], polygon['all_points_y'][i]] for i in range(0, len(polygon['all_points_x']))]\n",
    "\n",
    "        ground_truth_polygons.append(Polygon(gold_standard_polygon_xy))\n",
    "        \n",
    "    # Loop over all of the predicted and ground truth polygons and check if there\n",
    "    # is enough overlap to be a True Positive\n",
    "    for predicted_polygon in predicted_polygons:\n",
    "        for ground_truth_polygon in ground_truth_polygons:\n",
    "            \n",
    "            # make sure that the polygone is not self-intersecting\n",
    "            if not ground_truth_polygon.is_valid:\n",
    "                ground_truth_polygon = ground_truth_polygon.buffer(0)\n",
    "                \n",
    "            polygon_intersection = predicted_polygon.intersection(ground_truth_polygon).area\n",
    "            polygon_union = predicted_polygon.area + ground_truth_polygon.area - polygon_intersection\n",
    "        \n",
    "            if (polygon_intersection / polygon_union) >= 0.5:\n",
    "                sum_IoU += (polygon_intersection / polygon_union)\n",
    "                TP.append([ground_truth_polygon, predicted_polygon])\n",
    "\n",
    "    # Calculate false positives, false negatives and true positives and the PQ\n",
    "    # score.\n",
    "    FP = [polygon for polygon in predicted_polygons if polygon not in [item[1] for item in TP]]\n",
    "    FN = [polygon for polygon in ground_truth_polygons if polygon not in [item[0] for item in TP]]\n",
    "    \n",
    "    return {'TP': len(TP), 'FP': len(FP), 'FN': len(FN), 'IOU': sum_IoU}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cf838",
   "metadata": {},
   "source": [
    "## Evaluation Using PQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb778128",
   "metadata": {},
   "source": [
    "To evaluate on all the images, we have to load in the types of redacted text (which are also present in the json file). We then count the TP, FN, and FP for all images, and use this to calculate the final scores. We will work with dataframes here, as this is the easiest way to assign labels to the different pages. For the definition of the PQ metric, please see the original paper from Kirilov et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bad7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(os.path.join('..', 'datasets', 'data_complete.csv')).set_index('File')\n",
    "test_dir = os.path.join('..', 'datasets', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataframe(images_dir):\n",
    "    all_scores = {}\n",
    "    for filename in tqdm(os.listdir(images_dir)):\n",
    "        path = os.path.join(images_dir, filename)\n",
    "        image_scores = evaluate_detection(path)\n",
    "        all_scores[filename] = image_scores\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e8a5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 133/284 [03:16<04:25,  1.76s/it]C:\\Users\\kdmei\\anaconda3\\envs\\master\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\kdmei\\anaconda3\\envs\\master\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 284/284 [06:49<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "all_scores = evaluate_dataframe(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51bfcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(all_scores).T\n",
    "#add the scores and statistic to the dataframe\n",
    "complete_dataframe = data_csv.join(results, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3e4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "complete_dataframe.to_csv(os.path.join('results', 'ocr_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9bc9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculation(dataframe):\n",
    "    '''\n",
    "    The metric calculations as done in https://github.com/irlabamsterdam/TPDLTextRedaction/blob/main/notebooks/Experiments.ipynb\n",
    "    @param  pd.DataFrame    The dataframe for one class with the following columns { IOU, TP, FN, FP }\n",
    "                            where the IOU is the sum of IOU scores and the others a total count.\n",
    "    @return dict            The metric scores for this class\n",
    "    '''\n",
    "    \n",
    "    SQ = dataframe['IOU'].sum() / dataframe['TP'].sum()\n",
    "    RQ = dataframe['TP'].sum() / (dataframe['TP'].sum() + 0.5*dataframe['FN'].sum() + 0.5*dataframe['FP'].sum())\n",
    "    PQ = SQ*RQ\n",
    "    P = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FP'].sum())\n",
    "    R = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FN'].sum())\n",
    "    \n",
    "    number_of_segments =  dataframe['TP'] + dataframe['FN']\n",
    "    dataframe['num_segments'] = number_of_segments\n",
    "    return round(SQ, 2), round(RQ, 2) , round(PQ, 2), round(P, 2), round(R, 2), dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9cdf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the results table, where we group based on redaction type.\n",
    "type_results_table = {}\n",
    "for label, df in complete_dataframe.groupby('type'):\n",
    "    \n",
    "    # skip the 'no_annotations' type\n",
    "    if label == 'no_annotation': continue\n",
    "        \n",
    "    SQ, RQ, PQ, P, R, dataframe = metric_calculation(df)\n",
    "    redacted_type = df['type'].tolist()[0]\n",
    "    type_results_table[redacted_type] = {'PQ': PQ, 'SQ': SQ, 'RQ': RQ, 'P': P, 'R': R}\n",
    "\n",
    "SQ, RQ, PQ, P, R, dataframe = metric_calculation(complete_dataframe)\n",
    "type_results_table['total'] = {'PQ': PQ, 'SQ': SQ, 'RQ': RQ, 'P': P, 'R': R}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9178ec89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PQ</th>\n",
       "      <th>SQ</th>\n",
       "      <th>RQ</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>border</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gray</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PQ    SQ    RQ     P     R\n",
       "black   0.79  0.85  0.93  0.94  0.92\n",
       "border  0.52  0.88  0.59  0.83  0.45\n",
       "color   0.75  0.87  0.86  0.84  0.88\n",
       "gray    0.59  0.84  0.71  0.78  0.65\n",
       "total   0.68  0.86  0.79  0.87  0.73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the PQ results\n",
    "pd.DataFrame(type_results_table).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90624acc",
   "metadata": {},
   "source": [
    "## Time the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05982e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_algorithm(input_image_path):\n",
    "    '''\n",
    "    Time the image loading and model prediction\n",
    "    @param  string    The path to the image\n",
    "    @return dict      The times of the individual parts and total time\n",
    "    '''\n",
    "    \n",
    "    load_start = time.time()\n",
    "    input_image = load_image(input_image_path)\n",
    "    load_end = time.time()\n",
    "    # Do the preprocessing\n",
    "    pre_start = time.time()\n",
    "    image_text_pre, image_box_pre = text_preprocessing(input_image), redaction_box_preprocessing(input_image)\n",
    "    pre_end = time.time()\n",
    "    # Remove the text\n",
    "    remove_start = time.time()\n",
    "    image_without_text, _, text_boundaries = remove_text(image_text_pre, image_box_pre)\n",
    "    remove_end = time.time()\n",
    "    # First contour detection step\n",
    "    contour_det_start = time.time()\n",
    "    image_with_contours, contours = determine_contours(image_without_text)\n",
    "    contour_det_end = time.time()\n",
    "    # final contouring filtering step\n",
    "    final_step_start = time.time()\n",
    "    final_image_with_contours, final_contour_image, final_contours, _, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    final_step_end = time.time()\n",
    "    \n",
    "    times = {'loading': load_end-load_start,\n",
    "            'preprocessing': pre_end-pre_start,\n",
    "           'text_removal': remove_end-remove_start,\n",
    "           'contour detection': contour_det_end-contour_det_start,\n",
    "           'final filtering': final_step_end-final_step_start}\n",
    "    \n",
    "    times['total'] = sum(times.values())\n",
    "    \n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52b9bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 133/284 [03:10<04:00,  1.59s/it]C:\\Users\\kdmei\\anaconda3\\envs\\master\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\kdmei\\anaconda3\\envs\\master\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 284/284 [06:37<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# do this over all the images and average\n",
    "load_times = []\n",
    "preprocessing_times = []\n",
    "text_removal_times = []\n",
    "contour_detection_times = []\n",
    "final_filtering_times = []\n",
    "total_times = []\n",
    "\n",
    "# time the model for all test images\n",
    "for filename in tqdm(os.listdir(test_dir)):\n",
    "    image_path = os.path.join(test_dir, filename)\n",
    "    times = time_algorithm(image_path)\n",
    "    load_times.append(times['loading'])\n",
    "    preprocessing_times.append(times['preprocessing'])\n",
    "    text_removal_times.append(times['text_removal'])\n",
    "    contour_detection_times.append(times['contour detection'])\n",
    "    final_filtering_times.append(times['final filtering'])\n",
    "    total_times.append(times['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35889c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loading time is 0.064 seconds\n",
      "Average preprocessing time is 0.047 seconds\n",
      "Average text removal time is 1.258 seconds\n",
      "Average contour detection time is 0.020 seconds\n",
      "Average final filtering time is 0.006 seconds\n",
      "Average predicting time is 1.330 seconds\n",
      "Average total time is 1.395 seconds\n"
     ]
    }
   ],
   "source": [
    "# print the average times\n",
    "print(\"Average loading time is %.3f seconds\" % np.mean(load_times))\n",
    "print(\"Average preprocessing time is %.3f seconds\" % np.mean(preprocessing_times))\n",
    "print(\"Average text removal time is %.3f seconds\" % np.mean(text_removal_times))\n",
    "print(\"Average contour detection time is %.3f seconds\" % np.mean(contour_detection_times))\n",
    "print(\"Average final filtering time is %.3f seconds\" % np.mean(final_filtering_times))\n",
    "average_predicting_time = np.mean(preprocessing_times) + np.mean(text_removal_times) + np.mean(contour_detection_times) + np.mean(final_filtering_times)\n",
    "print(\"Average predicting time is %.3f seconds\" % average_predicting_time)\n",
    "print(\"Average total time is %.3f seconds\" % np.mean(total_times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
